<!DOCTYPE html>
<html>
    <head>
<title>Single AI BlogPost</title>
    </head>
    <body>
        <h1>Addressing concerns over the "Singularity" and AI's future dominance over humans</h1>
        <p>
            Much of the public's concern over AI stems from the idea that AI wil eventually advance to a point where
            it will be able to outperform humans in every way. This is known as the "Singularity". This idea is threatening in two ways.
            First of all, there's the threat of AI becoming so powerful and somehow it having misaligned incentives with humanity to the 
            point where it eventually wipes out humanity. The other concern is that we will become useless as we will be inferior to
            AI in every way, and thus have no meaning in our lives. In this post, I will address the first issue, and potential actions 
            that we as a society may take in order for such an undersirable outcome to never occur.

            The biggest mistake in my mind that people make when they voice this concern is that they treat AI as a singlular thing.
            Maybe this is because of movies or simply because it is hard for us to imagine an artificial being having some sort of individuality.
            Regardless, thinking logically, the future is likely to contain an extremely broad variety of AI, all of different functions,
            types, and intelligence levels. This eases many concerns regarding the power of AI. For example, a common illustration
            of how AI could destroy us is the example of how we destroy an anthill to build a highway, simply because it is in the way 
            of our goal. This is the argument that Elon Musk uses for being cautionary with the development of intelligent systems.
            The main idea behind this argument is that we will somehow create a system that is able to value humans less than their ultimate goal.
            However, we can only expect this sort of "erratic" behavior from AI with a more unsupervised objective, and therefore we can train more supervised, powerful
            AI with the goal of helping humanity. This is the idea behind the "Friendly AI" movement, which is a movement that aims to create AI that is beneficial to humanity.
            If we can accordingly figure out how to distribute power for these AI's, we can reduce the argument of our destruction.

            Critics would argue that even friendly AI's could be used to destroy us. For example, if we create a friendly AI with the goal of 
            minimizing the amount of long-term suffering in the world, then the AI may just destroy humanity so that noone ever has to 
            suffer again. Even if that is the case, we have three options. Option one would be to repeeat the process we just brought forth,
            where we create AI that is able to counter the AI we just created (or at least have some resistance). The second, obvious approach
            that people have been trying to solve since the beginning of mankind is to create a set of rules or goals that we can assign to any
            agent so that it acts in the best interest of humanity. This is the general study of philosophy and how to rule/lead a society,
            and is most definetly not a solved problem. The final option, that has been mostly ignored in my opinion is to give the 
            AI more of a fuzzy reward system, to simulate how humans treat the idea of "protecting humanity" for the most part. This 
            makes the most sense in my mind, mainly because it reshapes the problem. The problem has traditionally been "I intuitively know 
            how to protect humanity, but I'm not sure how to incentivize it as a system of rules with rewards and punishments that I can
            program an AI to follow so that I can speak its langauge of rules and systems and it can understand me". However, a new approach 
            posits "I intuitively understand how to protect humanity, and I don't know how to explain it to AI in a language that it understands,
            so I will create an AI that can speak my language and understand it in the same intuitive way that I do". This allows for us 
            to keep our understanding of what it means to "protect humanity" while aligning AI incentives with this goal. 

            One such way to do this would be a system such as a brain computer interface, where the AI's actions would be based off of the desires
            and wills of the human brain that it is "connected to" (literally or not). As far as I know, most reward functions are pretty well defined,
            but it is defintely possible to create a machine that thinks the same way as our brain (since worst case scenario, we have one example already, the brain)
            This fuzzy reward system makes some questions unanswerable (mathematically) since we can't calculate the relative rewards of each options.
            However, on the whole, we are able to know what is good and what isn't for ourselves and for society (for example, murder is bad)

            In fact, a more simple solution is to create an AI that can detect wheteher an action will have a detrimental effect against an individual
            or group of humans, and then have the AI act once that safety is ensured. Even with today's technology, this AI wouldn't be hard to make.
            Critics will argue there are various degrees of protection, and we would have to find a way to create a reward system for the AI
            so that it knows that for example a human getting punched is better than a human dying. I will do more research into harmful AI detection,
            especialy if we can simply actions into thoughts and reasoning.
        </p>
    </body>
</html>